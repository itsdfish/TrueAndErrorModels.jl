<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian Model Comparison · TrueAndErrorModels</title><meta name="title" content="Bayesian Model Comparison · TrueAndErrorModels"/><meta property="og:title" content="Bayesian Model Comparison · TrueAndErrorModels"/><meta property="twitter:title" content="Bayesian Model Comparison · TrueAndErrorModels"/><meta name="description" content="Documentation for TrueAndErrorModels."/><meta property="og:description" content="Documentation for TrueAndErrorModels."/><meta property="twitter:description" content="Documentation for TrueAndErrorModels."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="TrueAndErrorModels logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">TrueAndErrorModels</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../overview/">Model Overview</a></li><li><a class="tocitem" href="../parameter_estimation/">Bayesian Parameter Estimation</a></li><li class="is-active"><a class="tocitem" href>Bayesian Model Comparison</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Load-Packages"><span>Load Packages</span></a></li><li><a class="tocitem" href="#Define-Models"><span>Define Models</span></a></li><li><a class="tocitem" href="#TET1-Model"><span>TET1 Model</span></a></li><li><a class="tocitem" href="#Data-Generating-Model"><span>Data-Generating Model</span></a></li><li><a class="tocitem" href="#Estimate-Marginal-Log-Likelihood"><span>Estimate Marginal Log Likelihood</span></a></li><li><a class="tocitem" href="#Extract-marginal-log-likelihood"><span>Extract marginal log likelihood</span></a></li><li><a class="tocitem" href="#Compute-the-Bayes-Factor"><span>Compute the Bayes Factor</span></a></li><li class="toplevel"><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../turing_models/">Off-the-shelf Turing Models</a></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Bayesian Model Comparison</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian Model Comparison</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/itsdfish/TrueAndErrorModels.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/itsdfish/TrueAndErrorModels.jl/blob/main/docs/src/bayes_factor.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><img src="https://raw.githubusercontent.com/itsdfish/TrueAndErrorModels.jl/gh-pages/dev/assets/logo_readme.png" alt="drawing" width="900"/><h1 id="Bayesian-Model-Comparison"><a class="docs-heading-anchor" href="#Bayesian-Model-Comparison">Bayesian Model Comparison</a><a id="Bayesian-Model-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Model-Comparison" title="Permalink"></a></h1><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>In this tutorial, we will compare two True and Error model variants using the Bayes factor. One model variant imposes no restrictions on the error probability parameters, whereas the other model constrains the error probabilities to be equal. Computing the Bayes factor is challenging because it requires integrating over a potentially high dimensional parameter space. To compute Bayes factors, we will use a robust method called non-reversible parallel tempering (Bouchard-Côté et al., 2022) using the Julia package <a href="https://julia-tempering.github.io/Pigeons.jl/dev/">Pigeons.jl</a>. </p><h3 id="Bayes-Factor"><a class="docs-heading-anchor" href="#Bayes-Factor">Bayes Factor</a><a id="Bayes-Factor-1"></a><a class="docs-heading-anchor-permalink" href="#Bayes-Factor" title="Permalink"></a></h3><p>Before proceeding to the code, we provide a brief overview of the Bayes factor. Readers who are familiar with Bayes factors can skip this section. In Bayesian model comparison, the Bayes factor allows one to compare the probability of the data under two different models while taking into account model flexibility stemming all sources, including the number of parameters, functional form, and prior distribution. Thus, it provides a way to balance model fit and model flexibility into a single index. One important fact to keep in mind is that Bayes factors can be sensitive to the choice prior distributions over parameters. Sensitivity to prior distributions over parameters might be desireable depending on one&#39;s goals and knowledge of the models under consideration. </p><p>The Bayes factor is the likelihood of the data <span>$\mathbf{Y} = \left[y_1,y_2, \dots, y_n\right]$</span> under model <span>$\mathcal{M}_i$</span> vs. model <span>$\mathcal{M}_j$</span>. The relationship between the Bayes Factor and the posterior of odds of <span>$\mathcal{M}_i$</span> vs. <span>$\mathcal{M}_j$</span> can be stated as:</p><p><span>$\frac{\pi(\mathcal{M}_i \mid \mathbf{Y})}{\pi(\mathcal{M}_j \mid \mathbf{Y})} = \frac{\pi(\mathcal{M}_i)}{\pi(\mathcal{M}_j)} \mathrm{BF}_{i,j}.$</span></p><p>The term on the left hand side is the posterior odds of <span>$\mathcal{M}_i$</span> vs. <span>$\mathcal{M}_j$</span>, <span>$\pi$</span> is the posterior probability, the first term on the right hand side is the prior odds of <span>$\mathcal{M}_i$</span> vs. <span>$\mathcal{M}_j$</span>, and <span>$\mathrm{BF}_{i,j}$</span> is the Bayes factor for <span>$\mathcal{M}_i$</span> vs. <span>$\mathcal{M}_j$</span>.  In the equation above, <span>$\mathrm{BF}_{i,j}$</span> functions as a conversion factor between prior odds and posterior odds. Thus,  the Bayes factor is as the factor by which prior odds must be updated in light of the data. This interpretation is important because demonstrates that the prior odds should be updated by the same factor even if there is disagreement over the prior odds. The Bayes factor can also be written as the ratio of marginal likelihoods as follows: </p><p><span>$\mathrm{BF}_{i,j} = \frac{f(\mathbf{Y} \mid \mathcal{M}_i)}{f(\mathbf{Y} \mid \mathcal{M}_j)}$</span>,</p><p>where <span>$f$</span> is the likelihood function of <span>$\mathcal{M}_i$</span>, and the marginal likelihood of <span>$\mathcal{M}_i$</span> is given by:</p><p><span>$f(\mathbf{Y} \mid \mathcal{M}_i) = \int_{\boldsymbol{\theta}\in \boldsymbol{\Theta}_i} f(\mathbf{Y} \mid \boldsymbol{\theta}, \mathcal{M}_i) \pi(\boldsymbol{\theta} \mid \mathcal{M}_i) d \boldsymbol{\theta}$</span>.</p><p>In the equation above, <span>$\boldsymbol{\Theta}_i$</span> is the parameter space for <span>$\mathcal{M}_i$</span> and <span>$\boldsymbol{\theta} \in \boldsymbol{\Theta}$</span> is a vector of parameters. Under this interpretation, the marginal likelihood represents its average prior predictive ability of of <span>$\mathcal{M}_i$</span>. One benefit of the Bayes factor is that the marginal likelihood accounts for model flexibility because the density of the prior distribution must be &quot;rationed&quot; across the parameter space (i.e., must integrate to 1). Consequentially, the predictions of a model with a diffuse distribution in a high dimensional parameter space will be penalized due to its low prior density. </p><h2 id="Load-Packages"><a class="docs-heading-anchor" href="#Load-Packages">Load Packages</a><a id="Load-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Packages" title="Permalink"></a></h2><p>Before proceeding, we will load the required packages.</p><pre><code class="language-julia hljs">using MCMCChains
using Pigeons
using Random
using TrueAndErrorModels
using Turing</code></pre><h2 id="Define-Models"><a class="docs-heading-anchor" href="#Define-Models">Define Models</a><a id="Define-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Define-Models" title="Permalink"></a></h2><p>The following code blocks define the models along with their prior distributions using <a href="https://turinglang.org/stable/">Turing.jl</a>. Notice that the models are identical except constraints imposed on the error probabilities <span>$\epsilon_i$</span>.</p><h3 id="TET4-Model"><a class="docs-heading-anchor" href="#TET4-Model">TET4 Model</a><a id="TET4-Model-1"></a><a class="docs-heading-anchor-permalink" href="#TET4-Model" title="Permalink"></a></h3><p>The TET4 model is described in detail on the page called <a href="https://itsdfish.github.io/TrueAndErrorModels.jl/dev/overview/">model overview</a>. The <em>4</em> in TET4 refers to the number of error probability parameters, which are described below. The TET4 model has four true preference state parameters which collectively form the joint probability distribution over preference states <span>$R_1,R_2$</span>, <span>$R_1,S_2, $S_1,R_2$</span>, <span>$S_1S_2$</span>, where R represents a true preference for the risky option, S represents a true preference for the safe option, and the subscript corresponds to choice set. The joint preference states are:</p><ul><li><span>$p_{\mathrm{R_1R_2}}$</span>: the probability of prefering the risky option in both choice sets</li><li><span>$p_{\mathrm{R_1S_2}}$</span>: the probability of prefering the risky option in the first choice set and prefering the safe option in the second choice set</li><li><span>$p_{\mathrm{S_1R_2}}$</span>: the probability of prefering the safe option in the first choice set and prefering the risky option in the second choice set</li><li><span>$p_{\mathrm{S_1S_2}}$</span>: the probability of prefering the safe option in both choice sets</li></ul><p>subject to the constraint that <span>$p_{\mathrm{R_1R_2}} + p_{\mathrm{R_1S_2}} + p_{\mathrm{S_1R_2}} + p_{\mathrm{S_1S_2}} = 1$</span>, and <span>$p_i \geq 0, \forall i$</span>. As its namesake implies, the TET4 model also has four error probability parameters:</p><ul><li><span>$\epsilon_{\mathrm{S}_1}$</span>: the error probability of selecting <span>$\mathcal{S}_1$</span> given that <span>$\mathcal{R}_1$</span> is prefered.</li><li><span>$\epsilon_{\mathrm{S}_2}$</span>: the error probability of selecting <span>$\mathcal{S}_2$</span> given that <span>$\mathcal{R}_2$</span> is prefered.</li><li><span>$\epsilon_{\mathrm{R}_1}$</span>: the error probability of selecting <span>$\mathcal{R}_1$</span> given that <span>$\mathcal{S}_1$</span> is prefered.</li><li><span>$\epsilon_{\mathrm{R}_2}$</span>: the error probability of selecting <span>$\mathcal{R}_2$</span> given that <span>$\mathcal{S}_2$</span> is prefered.</li></ul><p>The only constraint is that <span>$\epsilon_i \in [0, .50],\forall i$</span>. The TET4 model is automatically loaded when Turing is loaded into your Julia session. The <code>tet4_model</code> function accepts a vector of response frequencies. The prior distributions are as follows:</p><p><span>$\mathbf{p} \sim \mathrm{Dirichlet}([1,1,1,1])$</span></p><p><span>$\boldsymbol{\epsilon} \sim \mathrm{Uniform}(0, .5)$</span></p><p>where <span>$\mathbf{p}$</span> is a vector of four preference state parameters, and <span>$\boldsymbol{epsilon}$</span> is a vector of error probabilities. </p><h2 id="TET1-Model"><a class="docs-heading-anchor" href="#TET1-Model">TET1 Model</a><a id="TET1-Model-1"></a><a class="docs-heading-anchor-permalink" href="#TET1-Model" title="Permalink"></a></h2><p>As the name implies, the TET1 model constrains all error probability parameters to be equal:</p><p><span>$\epsilon = \epsilon_{\mathrm{S}_1} = \epsilon_{\mathrm{S}_S} = \epsilon_{\mathrm{R}_1} =\epsilon_{\mathrm{R}_2}$</span></p><p>Otherwise, TET1 and TET4 are identical. The TET1 model is also automatically loaded when Turing is loaded into your Julia session. The <code>tet1_model</code> function accepts a vector of response frequencies, and using the following prior distributions over the parameters:</p><p><span>$\mathbf{p} \sim \mathrm{Dirichlet}([1,1,1,1])$</span></p><p><span>$\epsilon \sim \mathrm{Uniform}(0, .5),$</span></p><p>where <span>$\mathbf{p}$</span> is a vector of four preference state parameters, and error probability <span>$\epsilon$</span> is a scalar. </p><h2 id="Data-Generating-Model"><a class="docs-heading-anchor" href="#Data-Generating-Model">Data-Generating Model</a><a id="Data-Generating-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Generating-Model" title="Permalink"></a></h2><p>In our demonstration, we will use the TET1 as the data-generating model. In the code block below, we will create a model object and generate 2 simulated responses from all 100 simulated subjects for a total of 200 responses. In this model, we assume that the probability of a true preference state RR is relatively high, and the probability of other preference states decreases as they become more difference from RR:</p><ul><li><span>$p_{\mathrm{R_1R_2}} = .65$</span></li><li><span>$p_{\mathrm{R_1S_2}} = .15$</span></li><li><span>$p_{\mathrm{S_1R_2}} = .15$</span></li><li><span>$p_{\mathrm{S_1S_2}} = .05$</span></li></ul><p>In addition, our model assumes the error probabilities are constrained to be equal:</p><p><span>$\epsilon_{\mathrm{S}_1} = \epsilon_{\mathrm{S}_S} = \epsilon_{\mathrm{R}_1} =\epsilon_{\mathrm{R}_2} = .10$</span></p><pre><code class="language-julia hljs">Random.seed!(258)
dist = TrueErrorModel(; p = [0.65, .15, .15, .05], ϵ = fill(.10, 4))
data = rand(dist, 200)</code></pre><h2 id="Estimate-Marginal-Log-Likelihood"><a class="docs-heading-anchor" href="#Estimate-Marginal-Log-Likelihood">Estimate Marginal Log Likelihood</a><a id="Estimate-Marginal-Log-Likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Estimate-Marginal-Log-Likelihood" title="Permalink"></a></h2><p>The next step is to run the <code>pigeons</code> function to estimate the marginal log likelihood for each model. </p><h3 id="TET4"><a class="docs-heading-anchor" href="#TET4">TET4</a><a id="TET4-1"></a><a class="docs-heading-anchor-permalink" href="#TET4" title="Permalink"></a></h3><p>The code block below estimates the marginal log likelihood of the the TET4 model. This involves passing the <code>tet4_model</code> to the function <code>pigeons</code> along with the vector of response frequencies <code>data</code>.</p><pre><code class="language-julia hljs">pt_tet4 = pigeons(target=TuringLogPotential(tet4_model(data)), record=[traces])</code></pre><pre><code class="language-julia hljs">────────────────────────────────────────────────────────────────────────────
  scans        Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) 
────────── ────────── ────────── ────────── ────────── ────────── ──────────
        2       3.22      -47.6   0.000923      0.643          1          1 
        4       1.86      -39.9      0.265      0.793          1          1 
        8        3.6      -38.2      0.255        0.6          1          1 
       16        3.2      -39.2      0.403      0.645          1          1 
       32       3.51      -38.8       0.36       0.61          1          1 
       64       3.56      -39.6      0.441      0.605          1          1 
      128       3.78      -40.1      0.488       0.58          1          1 
      256       3.63      -39.4      0.482      0.596          1          1 
      512       3.61      -39.5      0.556      0.599          1          1 
 1.02e+03       3.56      -39.2      0.577      0.604          1          1 
────────────────────────────────────────────────────────────────────────────</code></pre><p>Below, we will change the numerical indices to more descriptive indices for ease of interpretation. The next line of code converts the output to an <code>Chain</code> object.</p><pre><code class="language-julia hljs">name_map = Dict(
    &quot;p[1]&quot; =&gt; &quot;pᵣᵣ&quot;,
    &quot;p[2]&quot; =&gt; &quot;pᵣₛ&quot;,
    &quot;p[3]&quot; =&gt; &quot;pₛᵣ&quot;,
    &quot;p[4]&quot; =&gt; &quot;pₛₛ&quot;,
    &quot;ϵ[1]&quot; =&gt; &quot;ϵₛ₁&quot;,
    &quot;ϵ[2]&quot; =&gt; &quot;ϵₛ₂&quot;,
    &quot;ϵ[3]&quot; =&gt; &quot;ϵᵣ₁&quot;,
    &quot;ϵ[4]&quot; =&gt; &quot;ϵᵣ₂&quot;,
)
chain_te4 = Chains(pt_tet4)
chain_te4 = replacenames(chain_te4, name_map)</code></pre><p>A summary of the MCMCChain is provided below.</p><pre><code class="language-julia hljs">Chains MCMC chain (1024×9×1 Array{Float64, 3}):

Iterations        = 1:1:1024
Number of chains  = 1
Samples per chain = 1024
parameters        = pᵣᵣ, pᵣₛ, pₛᵣ, pₛₛ, ϵₛ₁, ϵₛ₂, ϵᵣ₁, ϵᵣ₂
internals         = log_density

Summary Statistics
  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec 
      Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Missing 

         pᵣᵣ    0.5768    0.0824    0.0039   449.2267   570.8385    1.0027       missing
         pᵣₛ    0.1820    0.0662    0.0033   391.3331   750.0271    1.0000       missing
         pₛᵣ    0.1787    0.0579    0.0026   523.1174   740.9073    1.0002       missing
         pₛₛ    0.0625    0.0297    0.0012   618.7097   755.8075    0.9995       missing
         ϵₛ₁    0.0517    0.0314    0.0014   529.0317   866.9172    0.9995       missing
         ϵₛ₂    0.0571    0.0342    0.0017   418.1905   657.3602    1.0010       missing
         ϵᵣ₁    0.1995    0.1077    0.0048   514.3591   868.8844    1.0006       missing
         ϵᵣ₂    0.2706    0.1235    0.0065   372.7194   763.7186    1.0005       missing

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

         pᵣᵣ    0.4293    0.5165    0.5762    0.6383    0.7335
         pᵣₛ    0.0712    0.1317    0.1765    0.2275    0.3180
         pₛᵣ    0.0820    0.1335    0.1760    0.2202    0.2987
         pₛₛ    0.0179    0.0411    0.0576    0.0808    0.1294
         ϵₛ₁    0.0033    0.0245    0.0511    0.0754    0.1104
         ϵₛ₂    0.0033    0.0282    0.0566    0.0840    0.1238
         ϵᵣ₁    0.0162    0.1078    0.2056    0.2830    0.3887
         ϵᵣ₂    0.0232    0.1705    0.2894    0.3699    0.4639</code></pre><h3 id="TE1"><a class="docs-heading-anchor" href="#TE1">TE1</a><a id="TE1-1"></a><a class="docs-heading-anchor-permalink" href="#TE1" title="Permalink"></a></h3><p>As we did above, we will estimate the marginal log likelihood by passing <code>tet1_model</code> to the function<code>pigeons</code>. </p><pre><code class="language-julia hljs">pt_tet1 = pigeons(target=TuringLogPotential(te1_model(data)), record=[traces])</code></pre><pre><code class="language-julia hljs">────────────────────────────────────────────────────────────────────────────
  scans        Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) 
────────── ────────── ────────── ────────── ────────── ────────── ──────────
        2       3.18      -69.3   1.04e-16      0.647          1          1 
        4       2.11      -41.9    0.00298      0.766          1          1 
        8       3.41      -39.2      0.226      0.621          1          1 
       16       2.96      -38.6      0.364      0.671          1          1 
       32       3.71      -37.6      0.459      0.588          1          1 
       64       3.55      -38.3      0.505      0.605          1          1 
      128       3.42        -38      0.487       0.62          1          1 
      256       3.48      -38.1      0.556      0.613          1          1 
      512       3.28      -37.7      0.593      0.635          1          1 
 1.02e+03       3.41        -38      0.578      0.621          1          1 
────────────────────────────────────────────────────────────────────────────</code></pre><p>In the code block below, we will rename the parameters, and convert the output to an <code>Chain</code> object</p><pre><code class="language-julia hljs">name_map = Dict(
    &quot;p[1]&quot; =&gt; &quot;pᵣᵣ&quot;,
    &quot;p[2]&quot; =&gt; &quot;pᵣₛ&quot;,
    &quot;p[3]&quot; =&gt; &quot;pₛᵣ&quot;,
    &quot;p[4]&quot; =&gt; &quot;pₛₛ&quot;,
)
chain_te1 = Chains(pt_tet1)
chain_te1 = replacenames(chain_te1, name_map)</code></pre><p>The output below provides a summary of the chain.</p><pre><code class="language-julia hljs">Chains MCMC chain (1024×6×1 Array{Float64, 3}):

Iterations        = 1:1:1024
Number of chains  = 1
Samples per chain = 1024
parameters        = pᵣᵣ, pᵣₛ, pₛᵣ, pₛₛ, ϵ
internals         = log_density

Summary Statistics
  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ess_per_sec 
      Symbol   Float64   Float64   Float64     Float64     Float64   Float64       Missing 

         pᵣᵣ    0.7077    0.0351    0.0011   1106.2442   1055.5828    1.0000       missing
         pᵣₛ    0.1178    0.0261    0.0009    908.2753    965.3276    1.0009       missing
         pₛᵣ    0.1408    0.0268    0.0008   1036.7903    867.0191    0.9992       missing
         pₛₛ    0.0338    0.0140    0.0005    891.6153   1059.1575    1.0031       missing
           ϵ    0.0874    0.0115    0.0004    952.5670    803.6060    0.9995       missing

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

         pᵣᵣ    0.6364    0.6843    0.7065    0.7329    0.7712
         pᵣₛ    0.0717    0.0993    0.1165    0.1344    0.1746
         pₛᵣ    0.0923    0.1213    0.1402    0.1589    0.1964
         pₛₛ    0.0120    0.0235    0.0318    0.0422    0.0662
           ϵ    0.0668    0.0795    0.0867    0.0947    0.1114</code></pre><h2 id="Extract-marginal-log-likelihood"><a class="docs-heading-anchor" href="#Extract-marginal-log-likelihood">Extract marginal log likelihood</a><a id="Extract-marginal-log-likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Extract-marginal-log-likelihood" title="Permalink"></a></h2><p>In the following code block, the function <code>stepping_stone</code> extracts that marginal log likelihood for each model:</p><pre><code class="language-julia hljs">mll_tet1 = stepping_stone(pt_tet1)
mll_tet4 = stepping_stone(pt_tet4)</code></pre><h2 id="Compute-the-Bayes-Factor"><a class="docs-heading-anchor" href="#Compute-the-Bayes-Factor">Compute the Bayes Factor</a><a id="Compute-the-Bayes-Factor-1"></a><a class="docs-heading-anchor-permalink" href="#Compute-the-Bayes-Factor" title="Permalink"></a></h2><p>The bayes factor is obtained by exponentiating the difference between marginal log likelihoods. Recall that TET1 was the data-generating model.  As expected, the value of <code>3.39</code> indicates that the data are <code>3.39</code> times more likely under the data-generating model, TET1, than TET4.</p><pre><code class="language-julia hljs">bf = exp(mll_tet1 - mll_tet4)</code></pre><pre><code class="language-julia hljs">3.3948019100884617</code></pre><h1 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h1><p>Birnbaum, M. H., &amp; Quispe-Torreblanca, E. G. (2018). TEMAP2. R: True and error model analysis program in R. Judgment and Decision Making, 13(5), 428-440.</p><p>Lee, M. D. (2018). Bayesian methods for analyzing true-and-error models. Judgment and Decision making, 13(6), 622-635.</p><p>Syed, S., Bouchard-Côté, A., Deligiannidis, G., &amp; Doucet, A. (2022). Non-reversible parallel tempering: a scalable highly parallel MCMC scheme. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(2), 321-350.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../parameter_estimation/">« Bayesian Parameter Estimation</a><a class="docs-footer-nextpage" href="../turing_models/">Off-the-shelf Turing Models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Tuesday 17 September 2024 10:41">Tuesday 17 September 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
